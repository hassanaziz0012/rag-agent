{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"thenlper/gte-large\"\n",
    "\n",
    "def load_api_key():\n",
    "    \"\"\"Load the Gemini API key from environment variables or Colab.\"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "    return api_key\n",
    "\n",
    "\n",
    "def load_or_generate_embeddings(embedding_file=\"embedded_paragraphs.pkl\", book_file=\"book.md\"):\n",
    "    \"\"\"Load embeddings from file or generate them if not found.\"\"\"\n",
    "    try:\n",
    "        with open(embedding_file, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Embedded paragraphs not found. Generating embeddings...\")\n",
    "        return generate_embeddings(embedding_file, book_file)\n",
    "\n",
    "\n",
    "def generate_embeddings(embedding_file, book_file):\n",
    "    \"\"\"Generate embeddings for paragraphs in the book.\"\"\"\n",
    "    model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "\n",
    "    with open(book_file, \"r\") as file:\n",
    "        book = file.read()\n",
    "\n",
    "    paragraphs = book.split(\"\\n\\n\")\n",
    "\n",
    "    batch_size = 8  # smaller to avoid OOM\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(paragraphs), batch_size):\n",
    "        batch = paragraphs[i : i + batch_size]\n",
    "        embeddings = model.encode(batch, batch_size=len(batch), show_progress_bar=False, convert_to_tensor=True)\n",
    "        # Move to CPU for portable storage\n",
    "        all_embeddings.extend([e.cpu() for e in embeddings])\n",
    "\n",
    "    embedded_paragraphs = []\n",
    "    for i, p in enumerate(paragraphs):\n",
    "        embedded_paragraphs.append(\n",
    "            {\"id\": i, \"embedding\": all_embeddings[i], \"paragraph\": p}\n",
    "        )\n",
    "\n",
    "    with open(embedding_file, \"wb\") as file:\n",
    "        pickle.dump(embedded_paragraphs, file)\n",
    "\n",
    "    return embedded_paragraphs\n",
    "\n",
    "\n",
    "def get_embedding_model():\n",
    "    \"\"\"Load and return the sentence transformer model.\"\"\"\n",
    "    return SentenceTransformer(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "def search_paragraphs(query, embedded_paragraphs, model, top_k=5):\n",
    "    \"\"\"Search for the most relevant paragraphs based on the query.\"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True)\n",
    "\n",
    "    results = []\n",
    "    for i, paragraph in enumerate(embedded_paragraphs):\n",
    "        score = torch.cosine_similarity(query_embedding, paragraph[\"embedding\"]).item()\n",
    "        results.append({\"id\": i, \"paragraph\": paragraph[\"paragraph\"], \"score\": score})\n",
    "\n",
    "    return sorted(results, key=lambda x: x[\"score\"], reverse=True)[:top_k]\n",
    "\n",
    "\n",
    "def build_prompt(query, results):\n",
    "    \"\"\"Build the prompt for the LLM.\"\"\"\n",
    "    return f\"\"\"\n",
    "You are an LLM tasked with answering questions on the book \"Purpose and Profit\" by Dan Koe.\n",
    "\n",
    "This is how the author described the book:\n",
    "```\n",
    "Transform Your Relationship With Money & Discover Your Life's Work\n",
    "Money controls most people's lives, but it doesn't have to. Money is only superficial to the superficial. There is, in fact, a way to merge purpose and profit to create a life filled with work you don't want to escape from.\n",
    "```\n",
    "\n",
    "I'll give you the search query and the top 5 paragraphs from the book that are most relevant to the user's search query. Your job is to use those 5 paragraphs to answer the user's question as best as you can.\n",
    "\n",
    "- Do not deviate from the given paragraphs.\n",
    "- If you don't have an answer, say \"I don't know\".\n",
    "- Keep your answer concise and information-dense.\n",
    "\n",
    "Here is the user's search query:\n",
    "{query}\n",
    "\n",
    "Here are the top 5 paragraphs from the book that are most relevant to the user's search query:\n",
    "{results}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_response(api_key, prompt):\n",
    "    \"\"\"Generate a response from the LLM.\"\"\"\n",
    "    llm = genai.Client(api_key=api_key)\n",
    "    resp = llm.models.generate_content(model=\"gemini-3-flash-preview\", contents=prompt)\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the RAG pipeline.\"\"\"\n",
    "    print(\"[1/6] Loading API key...\")\n",
    "    api_key = load_api_key()\n",
    "    print(\"      API key loaded successfully.\")\n",
    "\n",
    "    print(\"[2/6] Loading or generating embeddings...\")\n",
    "    embedded_paragraphs = load_or_generate_embeddings()\n",
    "    print(f\"      Loaded {len(embedded_paragraphs)} embedded paragraphs.\")\n",
    "\n",
    "    print(\"[3/6] Loading embedding model...\")\n",
    "    model = get_embedding_model()\n",
    "    print(\"      Embedding model loaded.\")\n",
    "\n",
    "    print(\"[4/6] Searching for relevant paragraphs...\")\n",
    "    query = \"What is the purpose of life?\"\n",
    "    results = search_paragraphs(query, embedded_paragraphs, model)\n",
    "    print(f\"      Found {len(results)} relevant paragraphs.\")\n",
    "\n",
    "    print(\"[5/6] Search results:\")\n",
    "    for result in results:\n",
    "        print(f\"      ({result['score']:.4f}) {result['paragraph'][:80]}...\")\n",
    "        print()\n",
    "\n",
    "    print(\"[6/6] Generating LLM response...\")\n",
    "    prompt = build_prompt(query, results)\n",
    "    response = generate_response(api_key, prompt)\n",
    "    print(\"      Response from the LLM:\")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
